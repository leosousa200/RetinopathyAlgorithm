{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "from helpers.help import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('hr-dataset/full_df.csv')\n",
    "\n",
    "#Left eye\n",
    "# get the diagnostic of hypertensive retinopathy\n",
    "ds_hr_left = df[df['Left-Diagnostic Keywords'].str.contains('hypertensive retinopathy', na=False)]\n",
    "# get the diagnostic of diabetic retinopathy\n",
    "ds_dr_left = df[df['Left-Diagnostic Keywords'].str.contains('diabetic retinopathy', na=False)]\n",
    "# get the diagnostic of normal fundus\n",
    "ds_normal_left = df[df['Left-Diagnostic Keywords'] == 'normal fundus']\n",
    "\n",
    "\n",
    "#Right eye\n",
    "# get the diagnostic of hypertensive retinopathy\n",
    "ds_hr_right = df[df['Right-Diagnostic Keywords'].str.contains('hypertensive retinopathy', na=False)]\n",
    "# get the diagnostic of diabetic retinopathy\n",
    "ds_dr_right = df[df['Right-Diagnostic Keywords'].str.contains('diabetic retinopathy', na=False)]\n",
    "# get the diagnostic of normal fundus\n",
    "ds_normal_right = df[df['Right-Diagnostic Keywords'] == 'normal fundus']\n",
    "\n",
    "\n",
    "# Specific dataframe\n",
    "# Left eye\n",
    "df_hr_left = ds_hr_left[['Left-Diagnostic Keywords', 'Left-Fundus']]\n",
    "df_dr_left = ds_dr_left[['Left-Diagnostic Keywords', 'Left-Fundus']]\n",
    "df_normal_left = ds_normal_left[['Left-Diagnostic Keywords', 'Left-Fundus']]\n",
    "\n",
    "# Right eye\n",
    "df_hr_right = ds_hr_right[['Right-Diagnostic Keywords', 'Right-Fundus']]\n",
    "df_dr_right = ds_dr_right[['Right-Diagnostic Keywords', 'Right-Fundus']]\n",
    "df_normal_right = ds_normal_right[['Right-Diagnostic Keywords', 'Right-Fundus']]\n",
    "\n",
    "\n",
    "# Droping class\n",
    "# Left eye\n",
    "df_hr_left = df_hr_left.drop('Left-Diagnostic Keywords', axis=1)\n",
    "df_dr_left = df_dr_left.drop('Left-Diagnostic Keywords', axis=1)\n",
    "df_normal_left = df_normal_left.drop('Left-Diagnostic Keywords', axis=1)\n",
    "# Right eye\n",
    "df_hr_right = df_hr_right.drop('Right-Diagnostic Keywords', axis=1)\n",
    "df_dr_right = df_dr_right.drop('Right-Diagnostic Keywords', axis=1)\n",
    "df_normal_right = df_normal_right.drop('Right-Diagnostic Keywords', axis=1)\n",
    "\n",
    "\n",
    "# Undersample\n",
    "df_hr = pd.concat([df_hr_left, df_hr_right])\n",
    "df_dr = pd.concat([df_dr_left, df_dr_right])\n",
    "df_normal = pd.concat([df_normal_left, df_normal_right])\n",
    "\n",
    "df_hr_downsampled = resample(df_hr, replace=False, n_samples=200, random_state=10)\n",
    "df_dr_downsampled = resample(df_dr, replace=False, n_samples=165, random_state=10)\n",
    "df_normal_downsampled = resample(df_normal, replace=False, n_samples=220, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open Diabetic Retinopathy dataset\n",
    "path = os.path.join(os.getcwd(),'hr-dataset/preprocessed_images')\n",
    "\n",
    "# 0 - Normal\n",
    "# 1 - Diabetic Rethinopaty\n",
    "# 2 - Hipertensive Rethinopaty\n",
    "\n",
    "# roam Hipertensive rethinopaty\n",
    "array = []\n",
    "detailPath = \"\"\n",
    "for index, row in df_hr_downsampled.iterrows():\n",
    "    if type(row['Left-Fundus']) != float:\n",
    "        detailPath = os.path.join(path,row['Left-Fundus'])\n",
    "    else:\n",
    "        detailPath = os.path.join(path,row['Right-Fundus'])\n",
    "    if(os.path.exists(detailPath)):\n",
    "        array.append([detailPath,2])\n",
    "\n",
    "\n",
    "# roam Diabeic rethinopaty\n",
    "for index, row in df_dr_downsampled.iterrows():\n",
    "    if type(row['Left-Fundus']) != float:\n",
    "        detailPath = os.path.join(path,row['Left-Fundus'])\n",
    "    else:\n",
    "        detailPath = os.path.join(path,row['Right-Fundus'])\n",
    "    if(os.path.exists(detailPath)):\n",
    "        array.append([detailPath,1])\n",
    "\n",
    "# roam no rethinopaty\n",
    "for index, row in df_normal_downsampled.iterrows():\n",
    "    if type(row['Left-Fundus']) != float:\n",
    "        detailPath = os.path.join(path,row['Left-Fundus'])\n",
    "    else:\n",
    "        detailPath = os.path.join(path,row['Right-Fundus'])\n",
    "    if(os.path.exists(detailPath)):\n",
    "        array.append([detailPath,0])\n",
    "\n",
    "\n",
    "    \n",
    "# transforms the array into nparray\n",
    "dataset=np.array(array)\n",
    "\n",
    "np.size(dataset,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "\n",
    "#One hot encode the labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset (to make a unbiased model)\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 20% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.2*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split. We use 20% here just match the test set size to validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)\n",
    "\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=16)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=16)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_train,y_train,batch=BATCH_SIZE)\n",
    "\n",
    "# input shape for the model\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]\n",
    "\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]\n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    features, labels = batch  # Unpack the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "model_tl = keras.models.load_model(\"model/model_transferlearning.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n"
     ]
    }
   ],
   "source": [
    "class_names = {0: 'Normal', 1: 'Diabetic Rethinopaty', 2: 'Hipertensive Rethinopaty'}\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    # Predict probabilities for each batch\n",
    "    y_test_proba = model_tl.predict(x_batch)\n",
    "\n",
    "    # Convert probabilities to predicted class labels (0, 1, or 2)\n",
    "    y_pred.extend(np.argmax(y_test_proba, axis=1))\n",
    "\n",
    "    # Convert true labels from one-hot encoding to class labels (0, 1, or 2)\n",
    "    y_true.extend(np.argmax(y_batch.numpy(), axis=1))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Transform numerical labels into class names\n",
    "y_pred_names = [class_names[label] for label in y_pred]\n",
    "y_true_names = [class_names[label] for label in y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_accuracy': 0.4322766661643982, 'loss': 1.0912703275680542}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[128   3   2]\n",
      " [ 82  12   0]\n",
      " [110   0  10]]\n",
      "\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  Normal       0.80      0.13      0.22        94\n",
      "    Diabetic Rethinopaty       0.83      0.08      0.15       120\n",
      "Hipertensive Rethinopaty       0.40      0.96      0.57       133\n",
      "\n",
      "                accuracy                           0.43       347\n",
      "               macro avg       0.68      0.39      0.31       347\n",
      "            weighted avg       0.66      0.43      0.33       347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_tl.evaluate(test_dataset, verbose=0,return_dict=True))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true_names, y_pred_names, target_names=list(class_names.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
