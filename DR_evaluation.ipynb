{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from helpers.help import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3662"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open Diabetic Retinopathy dataset\n",
    "path = os.path.join(os.getcwd(),'gaussian_ds')\n",
    "label_dict={'Mild':1,'Moderate':1,'Proliferate_DR':1,'Severe':1,'No_DR':0}\n",
    "\n",
    "# remove macOS file\n",
    "folders = os.listdir(path)\n",
    "# folders.remove('.DS_Store')\n",
    "\n",
    "# get all the samples\n",
    "array = []\n",
    "for i in folders:\n",
    "    detailPath = os.path.join(path,i)\n",
    "    for j in os.listdir(detailPath):\n",
    "        array.append([os.path.join(detailPath,j),label_dict[i.split('.')[0]]])\n",
    "\n",
    "# transforms the array into nparray\n",
    "dataset=np.array(array)\n",
    "\n",
    "np.size(dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "\n",
    "#One hot encode the labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset (to make a unbiased model)\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 10% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.1*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split. We use 11% here just match the test set size to validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#We use the helper function to convert the data into tensorflow dataset objects. Note that , the repeat flag needs\n",
    "#to be set only for the train set , which by default is true.\n",
    "#The buid_dataset is a custom function that returns tensor batches\n",
    "# Prepares the datasets for train, validation and testing\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=64)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=64)\n",
    "\n",
    "BATCH_SIZE=32\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_train,y_train,batch=BATCH_SIZE)\n",
    "\n",
    "# input shape for the model\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model, trained before\n",
    "model_full_trained = keras.models.load_model(\"model/model_baseline.keras\")\n",
    "\n",
    "model_al = keras.models.load_model(\"model/model_al.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_full = []\n",
    "y_true_full = []\n",
    "y_proba_full = []\n",
    "\n",
    "class_names = {0: 'No DR', 1: 'DR'}\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    # Predict probabilities for each batch\n",
    "    y_test_proba_full = model_full_trained.predict(x_batch)\n",
    "\n",
    "    y_proba_full.extend(y_test_proba_full)\n",
    "\n",
    "    # Convert probabilities to class labels (for softmax output)\n",
    "    y_pred_full.extend(np.argmax(y_test_proba_full, axis=1))\n",
    "\n",
    "    # Convert true labels from one-hot encoding to class labels\n",
    "    y_true_full.extend(np.argmax(y_batch.numpy(), axis=1))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_pred_full = np.array(y_pred_full)\n",
    "y_true_full = np.array(y_true_full)\n",
    "\n",
    "# Transform 0 and 1 into class names\n",
    "y_pred_full_names = [class_names[label] for label in y_pred_full]\n",
    "y_true_full_names = [class_names[label] for label in y_true_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Active Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_al = []\n",
    "y_true_al = []\n",
    "y_proba_al = []\n",
    "\n",
    "# iterate over the batch\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    \n",
    "    # prediction of the model\n",
    "    y_test_proba_al = model_al.predict(x_batch)\n",
    "\n",
    "    y_proba_al.extend(y_test_proba_al)\n",
    "\n",
    "    # convert into the most likely class\n",
    "    y_pred_al.extend(np.argmax(y_test_proba_al, axis=1))\n",
    "\n",
    "    # convert the true list into a value for this x\n",
    "    y_true_al.extend(np.argmax(y_batch.numpy(), axis=1))\n",
    "\n",
    "y_pred_al = np.array(y_pred_al)\n",
    "y_true_al = np.array(y_true_al)\n",
    "\n",
    "# numerical values to strings\n",
    "y_pred_al_names = [class_names[label] for label in y_pred_al]\n",
    "y_true_al_names = [class_names[label] for label in y_true_al]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of full trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_accuracy': 0.9535518884658813, 'loss': 0.13201043009757996}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[188   7]\n",
      " [ 10 161]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          DR       0.96      0.94      0.95       171\n",
      "       No DR       0.95      0.96      0.96       195\n",
      "\n",
      "    accuracy                           0.95       366\n",
      "   macro avg       0.95      0.95      0.95       366\n",
      "weighted avg       0.95      0.95      0.95       366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_full_trained.evaluate(test_dataset, verbose=0,return_dict=True))\n",
    "conf_matrix = confusion_matrix(y_true_full, y_pred_full)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true_full_names, y_pred_full_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366, 2) (366, 2)\n",
      "Macro-Averaged ROC-AUC Score: 0.9897\n"
     ]
    }
   ],
   "source": [
    "# do one-hot enconding\n",
    "y_true_one_hot = tf.keras.utils.to_categorical(y_true_full, num_classes=2)\n",
    "\n",
    "\n",
    "y_proba = np.array(y_proba_full)  \n",
    "\n",
    "print(y_true_one_hot.shape ,y_proba.shape)\n",
    "\n",
    "# ROC-AUC score for each class\n",
    "auc = roc_auc_score(y_true_one_hot, y_proba, average='macro', multi_class='ovr')\n",
    "\n",
    "print(f\"Macro-Averaged ROC-AUC Score: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of AL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categorical_accuracy': 0.9535518884658813, 'loss': 0.21879823505878448}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[188   7]\n",
      " [ 10 161]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       No DR       0.96      0.94      0.95       171\n",
      "          DR       0.95      0.96      0.96       195\n",
      "\n",
      "    accuracy                           0.95       366\n",
      "   macro avg       0.95      0.95      0.95       366\n",
      "weighted avg       0.95      0.95      0.95       366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_al.evaluate(test_dataset, verbose=0,return_dict=True))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true_al, y_pred_al)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true_al_names, y_pred_al_names, target_names=list(class_names.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC AL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366, 2) (366, 2)\n",
      "Macro-Averaged ROC-AUC Score: 0.9786\n"
     ]
    }
   ],
   "source": [
    "# do one-hot enconding\n",
    "y_true_one_hot = tf.keras.utils.to_categorical(y_true_al, num_classes=2)\n",
    "\n",
    "\n",
    "y_proba_al = np.array(y_proba_al)  \n",
    "\n",
    "print(y_true_one_hot.shape ,y_proba_al.shape)\n",
    "\n",
    "\n",
    "# ROC-AUC score for each class\n",
    "auc = roc_auc_score(y_true_one_hot, y_proba_al, average='macro', multi_class='ovr')\n",
    "\n",
    "print(f\"Macro-Averaged ROC-AUC Score: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
